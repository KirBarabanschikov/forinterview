{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет объявлений на Авито"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/avito_train.csv')\n",
    "test = pd.read_csv('data/avito_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>Category_name</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Эбу Renault / Nissan 1,5 DCI Delphi 2001-2008 год</td>\n",
       "      <td>Комплект ЭБУ (мозги, компьютер мотора, двигате...</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Утюг утп 1000 ватт СССР 1987 год</td>\n",
       "      <td>Продам/\\n Фото № 1-2 /\\n /\\nУтюг УТП 1000 ватт...</td>\n",
       "      <td>Бытовая техника</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Возвму машину с выкупом</td>\n",
       "      <td>Возьму машину в аренду с последующим выкупом н...</td>\n",
       "      <td>Предложение услуг</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Полусапожки</td>\n",
       "      <td>полусапожки в отличном состоянии, один раз оде...</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Босоножки кожаные</td>\n",
       "      <td>Кожаные(натур) босоножки Karlo Pasolini, 40 рр...</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Эбу Renault / Nissan 1,5 DCI Delphi 2001-2008 год   \n",
       "1                   Утюг утп 1000 ватт СССР 1987 год   \n",
       "2                            Возвму машину с выкупом   \n",
       "3                                        Полусапожки   \n",
       "4                                  Босоножки кожаные   \n",
       "\n",
       "                                         description  \\\n",
       "0  Комплект ЭБУ (мозги, компьютер мотора, двигате...   \n",
       "1  Продам/\\n Фото № 1-2 /\\n /\\nУтюг УТП 1000 ватт...   \n",
       "2  Возьму машину в аренду с последующим выкупом н...   \n",
       "3  полусапожки в отличном состоянии, один раз оде...   \n",
       "4  Кожаные(натур) босоножки Karlo Pasolini, 40 рр...   \n",
       "\n",
       "               Category_name  Category  \n",
       "0      Запчасти и аксессуары        10  \n",
       "1            Бытовая техника        21  \n",
       "2          Предложение услуг       114  \n",
       "3  Одежда, обувь, аксессуары        27  \n",
       "4  Одежда, обувь, аксессуары        27  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4234042, 4)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем пропуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            0\n",
       "description      2\n",
       "Category_name    0\n",
       "Category         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполняем их пустой строкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.fillna(\"\")\n",
    "train['title&description'] = train['title']+' '+train.description\n",
    "train.Category.nunique() #количество объектов для предсказания"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы извлечь признаки из текстовых даннных воспользуемся векторизатором CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "try_train = train.head(10000) #возьмем первые 10к строк\n",
    "\n",
    "try_vec = CountVectorizer()\n",
    "try_bow = try_vec.fit_transform(np.array(try_train['title&description'])) #ранее объединенный двумя признаками столбец"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем удалять из текста русские \"стопслова\", которые не несут смысла и замедляют пересорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим стемминг - отбросим аффиксы (окончания и суффиксы) у слов и приведем их всех к норме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('russian')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кастомный токенайзер - он перебирает слова, удаляет стопслова и начинющиеся на цифры и символы и проводит стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "superiority = [' '.join([stemmer.stem(w) for w in wordpunct_tokenize(t.lower()) if (w.isalpha() and w not in stopwords.words('russian'))]) for t in try_train['title&description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попытки ускорить процесс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# superiority = [' '.join([stemmer.stem(w) for w in list(set(wordpunct_tokenize(t.lower()))) if (w.isalpha() and w not in stopwords.words('russian'))]) for t in try_train['title&description']]\n",
    "# superiority = [' '.join([stemmer.stem(w) for w in list(OrderedDict.fromkeys(wordpunct_tokenize(t.lower()))) if (w.isalpha() and w not in stopwords.words('russian'))]) for t in try_train['title&description']]\n",
    "# superiority = [' '.join([stemmer.stem(w) for w in wordpunct_tokenize(t.lower()) if (w.isalpha() and stopwords.words('russian').count(w) < 1)]) for t in try_train['title&description']]\n",
    "superiority = [' '.join([stemmer.stem(w) for w in wordpunct_tokenize(t.lower()) if (w.isalpha() and w not in np.array(stopwords.words('russian')))]) for t in try_train['title&description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_vec = CountVectorizer()\n",
    "super_bow = super_vec.fit_transform(superiority)\n",
    "x_train, x_test, y_train, y_test = train_test_split(super_bow, try_train.Category, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x27399 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 263627 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_bow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим данные на векторизаторах CountVectorizer и Tfidf с использованием SGDClassifier и посчитаем точность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7148"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sgd = SGDClassifier(loss='modified_huber', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "\n",
    "sgd.fit(x_train, y_train)\n",
    "\n",
    "sgd_y_pred = sgd.predict(x_test)\n",
    "\n",
    "accuracy_score(y_test, sgd_y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова, встречающиеся в одном тексте и сильно реже в других, обладают наибольшей ценностью для данного текста, ровно как и слова, которые часто встречаются в каждом тексте. Для разделения таких слов можно использовать статистическую меру TF-IDF (term frequency–inverse document frequency), характеризующую важность слова для конкретного текста. Для каждого слова из текста $d$ рассчитывается относительная частота встречаемости в нем (Term Frequency):\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{C(t | d)}{\\sum\\limits_{k \\in d}C(k | d)},\n",
    "$$\n",
    "где $C(t | d)$ - число вхождений слова $t$ в текст $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7655"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "super_vec = TfidfVectorizer(max_df=0.9)\n",
    "super_bow = super_vec.fit_transform(superiority)\n",
    "x_train, x_test, y_train, y_test = train_test_split(super_bow, try_train.Category, test_size=0.2, random_state=1)\n",
    "\n",
    "sgd = SGDClassifier(loss='modified_huber', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "\n",
    "sgd.fit(x_train, y_train)\n",
    "\n",
    "sgd_y_pred = sgd.predict(x_test)\n",
    "\n",
    "accuracy_score(y_test, sgd_y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применение метода hashing trick, который случайно группирует признаки и складывает их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.748"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "super_vec = HashingVectorizer(n_features=30000)\n",
    "super_bow = super_vec.fit_transform(superiority)\n",
    "x_train, x_test, y_train, y_test = train_test_split(super_bow, try_train.Category, test_size=0.2, random_state=1)\n",
    "\n",
    "sgd = SGDClassifier(loss='modified_huber', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "\n",
    "sgd.fit(x_train, y_train)\n",
    "\n",
    "sgd_y_pred = sgd.predict(x_test)\n",
    "\n",
    "accuracy_score(y_test, sgd_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
